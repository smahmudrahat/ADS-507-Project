# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17EDUV5edKcswFizXA9_cbFgjuqapMDHk
"""
# converting tsv file to csv

import csv
import os
import sys
import numpy as np

import pandas as pd

import pymysql as mysql
import matplotlib.pyplot as plt
import pandas as pd
from sqlalchemy import create_engine
import pandas as pd
from sqlalchemy import create_engine
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Directory containing the TSV files
directory = "/Users/smsultanmahmudrahat/Downloads/department/Data_engineering/Project/imbd/"

# Increase the maximum field size limit
max_int = sys.maxsize
while True:
    # Decrease the max_int value by half each time to handle the OverflowError
    try:
        csv.field_size_limit(max_int)
        break
    except OverflowError:
        max_int = int(max_int / 2)

# Loop through all files in the directory
for filename in os.listdir(directory):
    # Check if the file is a TSV file
    if filename.endswith(".tsv"):
        tsv_path = os.path.join(directory, filename)
        csv_path = os.path.join(directory, filename.replace('.tsv', '.csv'))

        with open(tsv_path, 'r', newline='', encoding='utf-8') as infile, \
             open(csv_path, 'w', newline='', encoding='utf-8') as outfile:
            tsv_reader = csv.reader(infile, delimiter='\t')
            csv_writer = csv.writer(outfile)

            # Write each row from the TSV file into the CSV file
            for row in tsv_reader:
                csv_writer.writerow(row)

        print(f"Converted {filename} to CSV.")

print("Conversion of all TSV files to CSV completed.")



path_name = r"/content/drive/MyDrive/Colab_Notebooks/new_csv_file/name.basics.csv"
path_title = r"/content/drive/MyDrive/Colab_Notebooks/new_csv_file/title.basics.csv"
path_rating = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/title.ratings.csv"
path_akas = r"/content/drive/MyDrive/Colab_Notebooks/new_csv_file/title.akas.csv"
path_crew = r"/content/drive/MyDrive/Colab_Notebooks/new_csv_file/title.crew.csv"
path_episode = r"/content/drive/MyDrive/Colab_Notebooks/new_csv_file/title.episode.csv"
path_streaming = r"/content/drive/MyDrive/Colab_Notebooks/new_csv_file/new_csv_file/Final_Movie_streaming_kaggle.csv"
path_movie = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_Movie_Industry_kaggle.csv"

"""## Prepare df_title:"""

# set low memory
df_rating = pd.read_csv(path_rating, low_memory=False)

df_title = pd.read_csv(path_title, low_memory=False)

"""# Merge df_title and df_rating


"""

# Perform a left join on 'tconst' and keep only 'tconst' and 'averageRating' from df_rating
df_title = df_title.merge(df_rating[['tconst', 'averageRating']], on='tconst', how='left')

# Replace NaN values in 'averageRating' with 0
df_title['averageRating'] = df_title['averageRating'].fillna(0)

# Display the first few rows of the modified dataset
df_title.head()

"""# Filtering data only for streaming platform

"""

path_streaming = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_Movie_streaming_kaggle.csv"
df_streaming = pd.read_csv(path_streaming)
path_movie = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_Movie_Industry_kaggle.csv"
df_movie = pd.read_csv(path_movie)

# Combining tconst values from df_movie and df_streaming into a single set
tconst_combined = set(df_movie['tconst']).union(set(df_streaming['tconst']))

"""# Preprocessing df_title dataset


"""

# Remove the 'endYear' column and replace '\N' with NaN
df_title.drop(columns=['endYear','originalTitle','isAdult','endYear','runtimeMinutes'], inplace=True)
df_title.replace('\\N', pd.NA, inplace=True)

# Step 1: Delete all rows with missing values
df_title.dropna(inplace=True)

# Split the 'genres' column into separate rows
# Convert the 'genres' string into a list of genres
df_title['genres'] = df_title['genres'].str.split(',')

#Keep only the titleType 'movie'
df_title = df_title[df_title['titleType'] == 'movie']

# Keep movies from 1990 to present
#First, ensure 'startYear' is an integer

# Convert 'startYear' to int and filter
df_title['startYear'] = df_title['startYear'].astype(int)
df_title = df_title[df_title['startYear'] >= 1980]

#now, we 'explode' the genres column
df_title = df_title.explode('genres')

print(f'Before filtering, number of rows: {df_title.shape[0]}')

# Filtering df_title_small for tconst values that exist in the combined set
df_title = df_title[df_title['tconst'].isin(tconst_combined)]

# Displaying the filtered dataframe
print(f'after filtering, number of rows: {df_title.shape[0]}')

# display the first few rows of the modified dataset
df_title.head()

# create a dataframe for movies from 1980 to 1989
df_1980s = df_title[(df_title['startYear'] >= 1980) & (df_title['startYear'] <= 1989)]

# create a dataframe for movies from 1990 to 1999
df_1990s = df_title[(df_title['startYear'] >= 1990) & (df_title['startYear'] <= 1999)]

# Create a dataframe for movies from 2000 to 2009
df_2000s = df_title[(df_title['startYear'] >= 2000) & (df_title['startYear'] <= 2009)]

# Ccreate a dataframe for movies from 2010 to 2019
df_2010s = df_title[(df_title['startYear'] >= 2010) & (df_title['startYear'] <= 2019)]

# Create a dataframe for movies from 2020 to the present
df_2020s = df_title[df_title['startYear'] >= 2020]

# Group by 'tconst' and other columns, and aggregate genres into a single string
df_1980s = df_1980s.groupby(['tconst', 'titleType', 'primaryTitle', 'startYear', 'averageRating'])['genres'].apply(','.join).reset_index()
df_1990s = df_1990s.groupby(['tconst', 'titleType', 'primaryTitle', 'startYear', 'averageRating'])['genres'].apply(','.join).reset_index()
df_2000s = df_2000s.groupby(['tconst', 'titleType', 'primaryTitle', 'startYear', 'averageRating'])['genres'].apply(','.join).reset_index()
df_2010s = df_2010s.groupby(['tconst', 'titleType', 'primaryTitle', 'startYear', 'averageRating'])['genres'].apply(','.join).reset_index()
df_2020s = df_2020s.groupby(['tconst', 'titleType', 'primaryTitle', 'startYear', 'averageRating'])['genres'].apply(','.join).reset_index()

df_1980s.to_csv(r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/1980s_movie.basics.csv", index=False)
df_1990s.to_csv(r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/1990s_movie.basics.csv", index=False)
df_2000s.to_csv(r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/2000s_movie.basics.csv", index=False)
df_2010s.to_csv(r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/2010s_movie.basics.csv", index=False)
df_2020s.to_csv(r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/2020s_movie.basics.csv", index=False)

# Convert the lists of dictionaries to pandas DataFrames
df_1980s = pd.DataFrame(df_1980s)
df_1990s = pd.DataFrame(df_1990s)
df_2000s = pd.DataFrame(df_2000s)
df_2010s = pd.DataFrame(df_2010s)
df_2020s = pd.DataFrame(df_2020s)

# Concatenate the DataFrames into one DataFrame
df_title = pd.concat([df_1980s, df_1990s, df_2000s,df_2010s,df_2020s], ignore_index=True)

df_title.to_csv(r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/title_movie.basics.csv", index=False)

print(df_1980s.shape)
print(df_1990s.shape)
print(df_2010s.shape)
print(df_2020s.shape)

"""# df_name"""

df_name = pd.read_csv(path_name, low_memory=False)

# replace '\N' with NaN
df_name.replace('\\N', np.nan, inplace=True)

# delete missing values
df_name.dropna(subset=['primaryProfession'], inplace=True)


# convert 'deathYear' to numeric (float) and handle NaNs
df_name['deathYear'] = pd.to_numeric(df_name['deathYear'], errors='coerce')

# Filter out rows where 'deathYear' is before 2010 or NaN
df_name = df_name[(df_name['deathYear'] >= 2010) | (df_name['deathYear'].isna())]


# Delete rows where 'deathYear' is before 2010
df_name = df_name[(df_name['deathYear'] >= 2010) | (df_name['deathYear'].isna())]

# Delete birthYear and deathYear
df_name.drop(columns=['birthYear','deathYear'], inplace=True)


#Split the 'primaryProfession' column
df_name['primaryProfession'] = df_name['primaryProfession'].str.split(',')

# Explode the 'primaryProfession' column
df_name = df_name.explode('primaryProfession')

df_name['knownForTitles'] = df_name['knownForTitles'].str.split(',')
df_name = df_name.explode('knownForTitles')

print(f'Before filtering, number of rows: {df_name.shape[0]}')


# Rename the column name and named as "tconst"
df_name.rename(columns={'knownForTitles': 'tconst'}, inplace=True)

# Filtering df_name for tconst values that exist in df_title
df_name = df_name[df_name['tconst'].isin(df_title['tconst'])]



# Step 6: Keep only certain professions
df_name = df_name[df_name['primaryProfession'].isin(['actor', 'actress', 'writer', 'director'])]

# Step 7: Merge 'actor' and 'actress' into a single category 'star'
df_name['primaryProfession'] = df_name['primaryProfession'].replace({'actor': 'star', 'actress': 'star'})

print(f'After filtering, number of rows: {df_name.shape[0]}')

df_star = df_name[df_name['primaryProfession'] == 'star'].copy()

# Create a separate dataframe for 'writer'
df_writer = df_name[df_name['primaryProfession'] == 'writer'].copy()

# Create a separate dataframe for 'director'
df_director = df_name[df_name['primaryProfession'] == 'director'].copy()

df_star.to_csv(r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_star_New.csv", index=False)
df_writer.to_csv(r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_writer_New.csv", index=False)
df_director.to_csv(r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_director_New.csv", index=False)

"""# Preparing external Source"""

movie_path = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/movies.csv"
streaming_path = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/MoviesOnStreamingPlatforms.csv"
df_movie = pd.read_csv(movie_path)
df_streaming = pd.read_csv(streaming_path)

"""# repairing"""

path_title = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/title.basics.csv"

df_title = pd.read_csv(path_title, low_memory=False)

# Remove the 'endYear' column and replace '\N' with NaN
df_title.drop(columns=['endYear','originalTitle','isAdult','endYear','runtimeMinutes'], inplace=True)
df_title.replace('\\N', pd.NA, inplace=True)

# Step 1: Delete all rows with missing values
df_title.dropna(inplace=True)

# Split the 'genres' column into separate rows
# Convert the 'genres' string into a list of genres
df_title['genres'] = df_title['genres'].str.split(',')

#Keep only the titleType 'movie'
df_title = df_title[df_title['titleType'] == 'movie']

# Keep movies from 1990 to present
#First, ensure 'startYear' is an integer

# Convert 'startYear' to int and filter
df_title['startYear'] = df_title['startYear'].astype(int)
df_title = df_title[df_title['startYear'] >= 1980]

#now, we 'explode' the genres column
df_title = df_title.explode('genres')

# display the first few rows of the modified dataset
df_title.head()

df_title_unique = df_title.drop_duplicates(subset='primaryTitle')
# Drop duplicates in df_title based on primaryTitle
df_title_unique = df_title.drop_duplicates(subset='primaryTitle')

# Merging df_movie with the deduplicated df_title
df_movie = df_movie.merge(df_title_unique[['tconst', 'primaryTitle']], left_on='name', right_on='primaryTitle', how='left')

# Drop rows where tconst is missing
df_movie.dropna(subset=['tconst'], inplace=True)


# Drop the extra 'primaryTitle' column
df_movie.drop(columns='primaryTitle', inplace=True)

# Shape of the df_movie
df_movie.shape

# number of missing value in df_movie[tconst]
df_movie['tconst'].isna().sum()
# tconst can be used as foreign key

# Location of path of 'streaming_path'
streaming_path = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/MoviesOnStreamingPlatforms.csv"

# Loading csv file
df_streaming = pd.read_csv(streaming_path)
# Filter df_streaming for movies from 1980 onwards
df_streaming_1980_onwards = df_streaming[df_streaming['Year'] >= 1980]

# Merge df_streaming with df_title
# Use a left join to keep all records in df_streaming and add the corresponding tconst from df_title
df_streaming_merged = df_streaming_1980_onwards.merge(
    df_title_unique[['tconst', 'primaryTitle']],
    left_on='Title',
    right_on='primaryTitle',
    how='left'
)

# Drop the extra 'primaryTitle' column from the merged DataFrame
df_streaming_merged.drop(columns='primaryTitle', inplace=True)


# Drop rows where tconst is missing
df_streaming_merged.dropna(subset=['tconst'], inplace=True)

# Converting 'Rotten Tomatoes' into Float datatype

df_streaming_merged['Rotten Tomatoes'] = df_streaming_merged['Rotten Tomatoes'].str.split('/').str[0].astype(float)

# Display the first few rows of the merged dataset
df_streaming_merged.head()

# Check consistency between two table
df_title[df_title['tconst'].isin(['tt11989890'])]

# number of missing value in df_movie[tconst]
df_streaming_merged['tconst'].isna().sum()
# tconst can be used as foreign key

"""# Save file on drive"""

df_movie.to_csv(r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_Movie_Industry_kaggle.csv", index=False)
df_streaming_merged.to_csv(r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_Movie_streaming_kaggle.csv", index=False)

"""# Building pipeline"""

path_title = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/title_movie.basics.csv"

path_movie = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_Movie_Industry_kaggle.csv"
path_streaming = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_Movie_streaming_kaggle.csv"

path_star = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_star_New.csv"
path_writer = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_writer_New.csv"
path_director = r"/content/drive/MyDrive/Colab_Notebooks/csv_file/new_csv_file/Final_director_New.csv"

df_title = pd.read_csv(path_title)

df_movie = pd.read_csv(path_movie)
df_streaming = pd.read_csv(path_streaming)

df_star = pd.read_csv(path_star)
df_writer = pd.read_csv(path_writer)
df_director = pd.read_csv(path_director)

print(f"Total rows of df_title: {df_title.shape[0]}")

print(f"Total rows of df_movie: {df_movie.shape[0]}")
print(f"Total rows of df_streaming: {df_streaming.shape[0]}")

print(f"Total rows of df_star: {df_star.shape[0]}")
print(f"Total rows of df_writer: {df_writer.shape[0]}")
print(f"Total rows of df_director: {df_director.shape[0]}")


## Transformation


conn=mysql.connect(host='imdb2.mysql.database.azure.com',port=int(3306),user= 'Team6Project',passwd='USDpassword507',db='imdb')




# Database connection parameters
db_username = 'Team6Project'
db_password = 'USDpassword507'
db_host = 'imdb2.mysql.database.azure.com'  
db_database = 'imdb'  # Database name

#path 
csv_file_path_title = '/Users/smsultanmahmudrahat/Downloads/department/Data_engineering/Project/imbd/code /new_csv_file 2/title_movie.basics.csv'
csv_file_path_director = '/Users/smsultanmahmudrahat/Downloads/department/Data_engineering/Project/imbd/code /new_csv_file 3/Final_director_New.csv'
csv_file_path_movie = '/Users/smsultanmahmudrahat/Downloads/department/Data_engineering/Project/imbd/code /new_csv_file 2/Final_Movie_Industry_kaggle.csv'
csv_file_path_streaming = '/Users/smsultanmahmudrahat/Downloads/department/Data_engineering/Project/imbd/code /new_csv_file 2/Final_Movie_streaming_kaggle.csv'
csv_file_path_star = '/Users/smsultanmahmudrahat/Downloads/department/Data_engineering/Project/imbd/code /new_csv_file 2/Final_star_New.csv'
csv_file_path_writer = '/Users/smsultanmahmudrahat/Downloads/department/Data_engineering/Project/imbd/code /new_csv_file 2/Final_writer_New.csv'

# Read CSV file using pandas
df_title = pd.read_csv(csv_file_path_title)

# Create a connection to your MySQL database
engine = create_engine(f'mysql+mysqlconnector://{db_username}:{db_password}@{db_host}/{db_database}')

# Insert data into the table, replace if the same primary key (tconst) exists
df_title.to_sql('Movies_Directory', con=engine, if_exists='append', index=False)

# Read CSV file using pandas
df_director = pd.read_csv(csv_file_path_director)

# Create a connection to your MySQL database
engine = create_engine(f'mysql+mysqlconnector://{db_username}:{db_password}@{db_host}/{db_database}')

# Insert data into the table, replace if the same primary key (tconst) exists
df_director.to_sql('Director', con=engine, if_exists='append', index=False)

# Read CSV file using pandas
df_star = pd.read_csv(csv_file_path_star)

# Create a connection to your MySQL database
engine = create_engine(f'mysql+mysqlconnector://{db_username}:{db_password}@{db_host}/{db_database}',pool_pre_ping=True)

# Insert data into the table, replace if the same primary key (tconst) exists
df_star.to_sql('star', con=engine, if_exists='append', index=False, chunksize=1000)


# Read CSV file using pandas
df_writer = pd.read_csv(csv_file_path_writer)

# Create a connection to your MySQL database
engine = create_engine(f'mysql+mysqlconnector://{db_username}:{db_password}@{db_host}/{db_database}')

# Insert data into the table, replace if the same primary key (tconst) exists
df_writer.to_sql('Writer', con=engine, if_exists='append', index=False)

# Read CSV file using pandas
df_movie = pd.read_csv(csv_file_path_movie)

# Create a connection to your MySQL database
engine = create_engine(f'mysql+mysqlconnector://{db_username}:{db_password}@{db_host}/{db_database}')

# Insert data into the table, replace if the same primary key (tconst) exists
df_movie.to_sql('Movies_Box_office', con=engine, if_exists='append', index=False)

import pandas as pd
from sqlalchemy import create_engine

# Read CSV file using pandas
df_streaming = pd.read_csv(csv_file_path_streaming)

# Drop the 'Unnamed: 0' column if it exists
if 'Unnamed: 0' in df_streaming.columns:
    df_streaming.drop('Unnamed: 0', axis=1, inplace=True)

# Rename columns to match the MySQL table structure
df_streaming.rename(columns={
    'Rotten Tomatoes': 'Rotten_Tomatoes',
    'Disney+': 'Disney_Plus',
    'Prime Video': 'Prime_Video'
}, inplace=True)

# Convert 'Rotten_Tomatoes' from '98/100' to 98
df_streaming['Rotten_Tomatoes'] = df_streaming['Rotten_Tomatoes'].str.split('/').str[0].astype(float)



# Create a connection to your MySQL database
engine = create_engine(f'mysql+mysqlconnector://{db_username}:{db_password}@{db_host}/{db_database}')

# Insert data into the table, replace if the same primary key (tconst) exists
df_streaming.to_sql('Streaming', con=engine, if_exists='append', index=False)

## visualiztion to check our pipeline is function or not

query = """
SELECT d.primaryName, COUNT(*) as NetflixMoviesCount
FROM Director d
JOIN Movies_Box_office m ON d.tconst = m.tconst
JOIN Streaming s ON m.tconst = s.tconst
WHERE s.Netflix = 1
GROUP BY d.primaryName
ORDER BY NetflixMoviesCount DESC
LIMIT 15;
"""
df = pd.read_sql(query,conn)
sns.set(style="whitegrid")
plt.figure(figsize=(12, 8))
barplot = sns.barplot(x='NetflixMoviesCount', y='primaryName', data=df, palette='coolwarm')

plt.xlabel('Number of Netflix Movies')
plt.ylabel('Director')
plt.title('Top 15 Directors with Most Movies on Netflix')

plt.show()

query = """
SELECT d.primaryName, COUNT(*) as NetflixMoviesCount
FROM Star d
JOIN Movies_Box_office m ON d.tconst = m.tconst
JOIN Streaming s ON m.tconst = s.tconst
WHERE s.Netflix = 1
GROUP BY d.primaryName
ORDER BY NetflixMoviesCount DESC
LIMIT 15;
"""
df = pd.read_sql(query,conn)


sns.set(style="whitegrid")
plt.figure(figsize=(12, 8))
barplot = sns.barplot(x='NetflixMoviesCount', y='primaryName', data=df, palette='coolwarm')

plt.xlabel('Number of Netflix Movies')
plt.ylabel('Star')
plt.title('Top 15 Stars with Most Movies on Netflix')

plt.show()
